{
 "metadata": {
  "name": "",
  "signature": "sha256:c3b106f0273da432b3c587351c462cc56d901f3b5a1b788ed4ee527ed51ad580"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img align=left src=\"files/images/pyspark-page1.svg\" width=500 height=250 />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img align=left src=\"files/images/pyspark-page2.svg\" width=500 height=500 />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Note: click on an image to view the corresponding pyspark docs"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# print Spark version\n",
      "print(\"pyspark version:\" + str(sc.version))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "pyspark version:1.2.2\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.map\">\n",
      "<img align=left src=\"files/images/pyspark-page3.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# map\n",
      "x = sc.parallelize([1,2,3]) # sc = spark context, parallelize creates an RDD from the passed object\n",
      "y = x.map(lambda x: (x,x**2))\n",
      "print(x.collect())  # collect copies RDD elements to a list on the driver\n",
      "print(y.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 2, 3]\n",
        "[(1, 1), (2, 4), (3, 9)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.flatMap\">\n",
      "<img align=left src=\"files/images/pyspark-page4.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# flatMap\n",
      "x = sc.parallelize([1,2,3])\n",
      "y = x.flatMap(lambda x: (x, 100*x, x**2))\n",
      "print(x.collect())\n",
      "print(y.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 2, 3]\n",
        "[1, 100, 1, 2, 200, 4, 3, 300, 9]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.mapPartitions\">\n",
      "<img align=left src=\"files/images/pyspark-page5.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# mapPartitions\n",
      "x = sc.parallelize([1,2,3], 2)\n",
      "def f(iterator): yield sum(iterator)\n",
      "y = x.mapPartitions(f)\n",
      "print(x.glom().collect())  # glom() flattens elements on the same partition\n",
      "print(y.glom().collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[1], [2, 3]]\n",
        "[[1], [5]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.mapPartitionsWithIndex\">\n",
      "<img align=left src=\"files/images/pyspark-page6.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# mapPartitionsWithIndex\n",
      "x = sc.parallelize([1,2,3], 2)\n",
      "def f(partitionIndex, iterator): yield (partitionIndex,sum(iterator))\n",
      "y = x.mapPartitionsWithIndex(f)\n",
      "print(x.glom().collect())  # glom() flattens elements on the same partition\n",
      "print(y.glom().collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[1], [2, 3]]\n",
        "[[(0, 1)], [(1, 5)]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.getNumPartitions\">\n",
      "<img align=left src=\"files/images/pyspark-page7.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# getNumPartitions\n",
      "x = sc.parallelize([1,2,3], 2)\n",
      "y = x.getNumPartitions()\n",
      "print(x.glom().collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[1], [2, 3]]\n",
        "2\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.filter\">\n",
      "<img align=left src=\"files/images/pyspark-page8.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# filter\n",
      "x = sc.parallelize([1,2,3])\n",
      "y = x.filter(lambda x: x%2 == 1)  # filters out even elements\n",
      "print(x.collect())\n",
      "print(y.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 2, 3]\n",
        "[1, 3]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.distinct\">\n",
      "<img align=left src=\"files/images/pyspark-page9.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# distinct\n",
      "x = sc.parallelize(['A','A','B'])\n",
      "y = x.distinct()\n",
      "print(x.collect())\n",
      "print(y.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['A', 'A', 'B']\n",
        "['A', 'B']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.sample\">\n",
      "<img align=left src=\"files/images/pyspark-page10.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# sample\n",
      "x = sc.parallelize(range(7))\n",
      "ylist = [x.sample(withReplacement=False, fraction=0.5) for i in range(5)] # call 'sample' 5 times\n",
      "print('x = ' + str(x.collect()))\n",
      "for cnt,y in zip(range(len(ylist)), ylist):\n",
      "    print('sample:' + str(cnt) + ' y = ' +  str(y.collect()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "x = [0, 1, 2, 3, 4, 5, 6]\n",
        "sample:0 y = [0, 2, 5, 6]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sample:1 y = [2, 6]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sample:2 y = [0, 4, 5, 6]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sample:3 y = [0, 2, 6]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sample:4 y = [0, 3, 4]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.takeSample\">\n",
      "<img align=left src=\"files/images/pyspark-page11.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# takeSample\n",
      "x = sc.parallelize(range(7))\n",
      "ylist = [x.takeSample(withReplacement=False, num=3) for i in range(5)]  # call 'sample' 5 times\n",
      "print('x = ' + str(x.collect()))\n",
      "for cnt,y in zip(range(len(ylist)), ylist):\n",
      "    print('sample:' + str(cnt) + ' y = ' +  str(y))  # no collect on y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "x = [0, 1, 2, 3, 4, 5, 6]\n",
        "sample:0 y = [0, 2, 6]\n",
        "sample:1 y = [6, 4, 2]\n",
        "sample:2 y = [2, 0, 4]\n",
        "sample:3 y = [5, 4, 1]\n",
        "sample:4 y = [3, 1, 4]\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.union\">\n",
      "<img align=left src=\"files/images/pyspark-page12.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# union\n",
      "x = sc.parallelize(['A','A','B'])\n",
      "y = sc.parallelize(['D','C','A'])\n",
      "z = x.union(y)\n",
      "print(x.collect())\n",
      "print(y.collect())\n",
      "print(z.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['A', 'A', 'B']\n",
        "['D', 'C', 'A']\n",
        "['A', 'A', 'B', 'D', 'C', 'A']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.intersection\">\n",
      "<img align=left src=\"files/images/pyspark-page13.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# intersection\n",
      "x = sc.parallelize(['A','A','B'])\n",
      "y = sc.parallelize(['A','C','D'])\n",
      "z = x.intersection(y)\n",
      "print(x.collect())\n",
      "print(y.collect())\n",
      "print(z.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['A', 'A', 'B']\n",
        "['A', 'C', 'D']\n",
        "['A']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.sortByKey\">\n",
      "<img align=left src=\"files/images/pyspark-page14.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# sortByKey\n",
      "x = sc.parallelize([('B',1),('A',2),('C',3)])\n",
      "y = x.sortByKey()\n",
      "print(x.collect())\n",
      "print(y.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('B', 1), ('A', 2), ('C', 3)]\n",
        "[('A', 2), ('B', 1), ('C', 3)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.sortBy\">\n",
      "<img align=left src=\"files/images/pyspark-page15.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# sortBy\n",
      "x = sc.parallelize(['Cat','Apple','Bat'])\n",
      "def keyGen(val): return val[0]\n",
      "y = x.sortBy(keyGen)\n",
      "print(y.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['Apple', 'Bat', 'Cat']\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.glom\">\n",
      "<img align=left src=\"files/images/pyspark-page16.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# glom\n",
      "x = sc.parallelize(['C','B','A'], 2)\n",
      "y = x.glom()\n",
      "print(x.collect()) \n",
      "print(y.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['C', 'B', 'A']\n",
        "[['C'], ['B', 'A']]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.cartesian\">\n",
      "<img align=left src=\"files/images/pyspark-page17.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# cartesian\n",
      "x = sc.parallelize(['A','B'])\n",
      "y = sc.parallelize(['C','D'])\n",
      "z = x.cartesian(y)\n",
      "print(x.collect())\n",
      "print(y.collect())\n",
      "print(z.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['A', 'B']\n",
        "['C', 'D']\n",
        "[('A', 'C'), ('A', 'D'), ('B', 'C'), ('B', 'D')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.groupBy\">\n",
      "<img align=left src=\"files/images/pyspark-page18.svg\" width=500 height=500 />\n",
      "<"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# groupBy\n",
      "x = sc.parallelize([1,2,3])\n",
      "y = x.groupBy(lambda x: 'A' if (x%2 == 1) else 'B' )\n",
      "print(x.collect())\n",
      "print([(j[0],[i for i in j[1]]) for j in y.collect()]) # y is nested, this iterates through it"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 2, 3]\n",
        "[('A', [1, 3]), ('B', [2])]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.pipe\">\n",
      "<img align=left src=\"files/images/pyspark-page19.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# pipe\n",
      "x = sc.parallelize(['A', 'Ba', 'C', 'AD'])\n",
      "y = x.pipe('grep -i \"A\"') # calls out to grep, may fail under Windows \n",
      "print(x.collect())\n",
      "print(y.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['A', 'Ba', 'C', 'AD']\n",
        "['A', 'Ba', 'AD']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.foreach\">\n",
      "<img align=left src=\"files/images/pyspark-page20.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# foreach\n",
      "from __future__ import print_function\n",
      "x = sc.parallelize([1,2,3])\n",
      "def f(el):\n",
      "    '''side effect: append the current RDD elements to a file'''\n",
      "    f1=open(\"./foreachExample.txt\", 'a+') \n",
      "    print(el,file=f1)\n",
      "\n",
      "open('./foreachExample.txt', 'w').close()  # first clear the file contents\n",
      "\n",
      "y = x.foreach(f) # writes into foreachExample.txt\n",
      "\n",
      "print(x.collect())\n",
      "print(y) # foreach returns 'None'\n",
      "# print the contents of foreachExample.txt\n",
      "with open(\"./foreachExample.txt\", \"r\") as foreachExample:\n",
      "    print (foreachExample.read())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 2, 3]\n",
        "None\n",
        "3\n",
        "1\n",
        "2\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.foreachPartition\">\n",
      "<img align=left src=\"files/images/pyspark-page21.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# foreachPartition\n",
      "from __future__ import print_function\n",
      "x = sc.parallelize([1,2,3],5)\n",
      "def f(parition):\n",
      "    '''side effect: append the current RDD partition contents to a file'''\n",
      "    f1=open(\"./foreachPartitionExample.txt\", 'a+') \n",
      "    print([el for el in parition],file=f1)\n",
      "\n",
      "open('./foreachPartitionExample.txt', 'w').close()  # first clear the file contents\n",
      "\n",
      "y = x.foreachPartition(f) # writes into foreachExample.txt\n",
      "\n",
      "print(x.glom().collect())\n",
      "print(y)  # foreach returns 'None'\n",
      "# print the contents of foreachExample.txt\n",
      "with open(\"./foreachPartitionExample.txt\", \"r\") as foreachExample:\n",
      "    print (foreachExample.read())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[], [1], [], [2], [3]]\n",
        "None\n",
        "[]\n",
        "[]\n",
        "[1]\n",
        "[2]\n",
        "[3]\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.collect\">\n",
      "<img align=left src=\"files/images/pyspark-page22.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# collect\n",
      "x = sc.parallelize([1,2,3])\n",
      "y = x.collect()\n",
      "print(x)  # distributed\n",
      "print(y)  # not distributed"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ParallelCollectionRDD[87] at parallelize at PythonRDD.scala:382\n",
        "[1, 2, 3]\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.reduce\">\n",
      "<img align=left src=\"files/images/pyspark-page23.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# reduce\n",
      "x = sc.parallelize([1,2,3])\n",
      "y = x.reduce(lambda obj, accumulated: obj + accumulated)  # computes a cumulative sum\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 2, 3]\n",
        "6\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.fold\">\n",
      "<img align=left src=\"files/images/pyspark-page24.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# fold\n",
      "x = sc.parallelize([1,2,3])\n",
      "neutral_zero_value = 0  # 0 for sum, 1 for multiplication\n",
      "y = x.fold(neutral_zero_value,lambda obj, accumulated: accumulated + obj) # computes cumulative sum\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 2, 3]\n",
        "6\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.aggregate\">\n",
      "<img align=left src=\"files/images/pyspark-page25.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# aggregate\n",
      "x = sc.parallelize([2,3,4])\n",
      "neutral_zero_value = (0,1) # sum: x+0 = x, product: 1*x = x\n",
      "seqOp = (lambda aggregated, el: (aggregated[0] + el, aggregated[1] * el)) \n",
      "combOp = (lambda aggregated, el: (aggregated[0] + el[0], aggregated[1] * el[1]))\n",
      "y = x.aggregate(neutral_zero_value,seqOp,combOp)  # computes (cumulative sum, cumulative product)\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[2, 3, 4]\n",
        "(9, 24)\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.max\">\n",
      "<img align=left src=\"files/images/pyspark-page26.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# max\n",
      "x = sc.parallelize([1,3,2])\n",
      "y = x.max()\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 3, 2]\n",
        "3\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.min\">\n",
      "<img align=left src=\"files/images/pyspark-page27.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# min\n",
      "x = sc.parallelize([1,3,2])\n",
      "y = x.min()\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 3, 2]\n",
        "1\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.sum\">\n",
      "<img align=left src=\"files/images/pyspark-page28.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# sum\n",
      "x = sc.parallelize([1,3,2])\n",
      "y = x.sum()\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 3, 2]\n",
        "6\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.count\">\n",
      "<img align=left src=\"files/images/pyspark-page29.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# count\n",
      "x = sc.parallelize([1,3,2])\n",
      "y = x.count()\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 3, 2]\n",
        "3\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.histogram\">\n",
      "<img align=left src=\"files/images/pyspark-page30.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# histogram (example #1)\n",
      "x = sc.parallelize([1,3,1,2,3])\n",
      "y = x.histogram(buckets = 2)\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 3, 1, 2, 3]\n",
        "([1, 2, 3], [2, 3])\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# histogram (example #2)\n",
      "x = sc.parallelize([1,3,1,2,3])\n",
      "y = x.histogram([0,0.5,1,1.5,2,2.5,3,3.5])\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 3, 1, 2, 3]\n",
        "([0, 0.5, 1, 1.5, 2, 2.5, 3, 3.5], [0, 0, 2, 0, 1, 0, 2])\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.mean\">\n",
      "<img align=left src=\"files/images/pyspark-page31.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# mean\n",
      "x = sc.parallelize([1,3,2])\n",
      "y = x.mean()\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 3, 2]\n",
        "2.0\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.variance\">\n",
      "<img align=left src=\"files/images/pyspark-page32.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# variance\n",
      "x = sc.parallelize([1,3,2])\n",
      "y = x.variance()  # divides by N\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 3, 2]\n",
        "0.666666666667\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.stdev\">\n",
      "<img align=left src=\"files/images/pyspark-page33.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# stdev\n",
      "x = sc.parallelize([1,3,2])\n",
      "y = x.stdev()  # divides by N\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 3, 2]\n",
        "0.816496580928\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.sampleStdev\">\n",
      "<img align=left src=\"files/images/pyspark-page34.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# sampleStdev\n",
      "x = sc.parallelize([1,3,2])\n",
      "y = x.sampleStdev() # divides by N-1\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 3, 2]\n",
        "1.0\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.sampleVariance\">\n",
      "<img align=left src=\"files/images/pyspark-page35.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# sampleVariance\n",
      "x = sc.parallelize([1,3,2])\n",
      "y = x.sampleVariance()  # divides by N-1\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 3, 2]\n",
        "1.0\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.countByValue\">\n",
      "<img align=left src=\"files/images/pyspark-page36.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# countByValue\n",
      "x = sc.parallelize([1,3,1,2,3])\n",
      "y = x.countByValue()\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 3, 1, 2, 3]\n",
        "defaultdict(<type 'int'>, {1: 2, 2: 1, 3: 2})\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.top\">\n",
      "<img align=left src=\"files/images/pyspark-page37.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# top\n",
      "x = sc.parallelize([1,3,1,2,3])\n",
      "y = x.top(num = 3)\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 3, 1, 2, 3]\n",
        "[3, 3, 2]\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.takeOrdered\">\n",
      "<img align=left src=\"files/images/pyspark-page38.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# takeOrdered\n",
      "x = sc.parallelize([1,3,1,2,3])\n",
      "y = x.takeOrdered(num = 3)\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 3, 1, 2, 3]\n",
        "[1, 1, 2]\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.take\">\n",
      "<img align=left src=\"files/images/pyspark-page39.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# take\n",
      "x = sc.parallelize([1,3,1,2,3])\n",
      "y = x.take(num = 3)\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 3, 1, 2, 3]\n",
        "[1, 3, 1]\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.first\">\n",
      "<img align=left src=\"files/images/pyspark-page40.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# first\n",
      "x = sc.parallelize([1,3,1,2,3])\n",
      "y = x.first()\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 3, 1, 2, 3]\n",
        "1\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.collectAsMap\">\n",
      "<img align=left src=\"files/images/pyspark-page41.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# collectAsMap\n",
      "x = sc.parallelize([('C',3),('A',1),('B',2)])\n",
      "y = x.collectAsMap()\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('C', 3), ('A', 1), ('B', 2)]\n",
        "{'A': 1, 'C': 3, 'B': 2}\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.keys\">\n",
      "<img align=left src=\"files/images/pyspark-page42.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# keys\n",
      "x = sc.parallelize([('C',3),('A',1),('B',2)])\n",
      "y = x.keys()\n",
      "print(x.collect())\n",
      "print(y.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('C', 3), ('A', 1), ('B', 2)]\n",
        "['C', 'A', 'B']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.values\">\n",
      "<img align=left src=\"files/images/pyspark-page43.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# values\n",
      "x = sc.parallelize([('C',3),('A',1),('B',2)])\n",
      "y = x.values()\n",
      "print(x.collect())\n",
      "print(y.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('C', 3), ('A', 1), ('B', 2)]\n",
        "[3, 1, 2]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.reduceByKey\">\n",
      "<img align=left src=\"files/images/pyspark-page44.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# reduceByKey\n",
      "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
      "y = x.reduceByKey(lambda agg, obj: agg + obj)\n",
      "print(x.collect())\n",
      "print(y.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('B', 1), ('B', 2), ('A', 3), ('A', 4), ('A', 5)]\n",
        "[('A', 12), ('B', 3)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.reduceByKeyLocally\">\n",
      "<img align=left src=\"files/images/pyspark-page45.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# reduceByKeyLocally\n",
      "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
      "y = x.reduceByKeyLocally(lambda agg, obj: agg + obj)\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('B', 1), ('B', 2), ('A', 3), ('A', 4), ('A', 5)]\n",
        "{'A': 12, 'B': 3}\n"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.countByKey\">\n",
      "<img align=left src=\"files/images/pyspark-page46.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# countByKey\n",
      "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
      "y = x.countByKey()\n",
      "print(x.collect())\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('B', 1), ('B', 2), ('A', 3), ('A', 4), ('A', 5)]\n",
        "defaultdict(<type 'int'>, {'A': 3, 'B': 2})\n"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.join\">\n",
      "<img align=left src=\"files/images/pyspark-page47.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# join\n",
      "x = sc.parallelize([('C',4),('B',3),('A',2),('A',1)])\n",
      "y = sc.parallelize([('A',8),('B',7),('A',6),('D',5)])\n",
      "z = x.join(y)\n",
      "print(x.collect())\n",
      "print(y.collect())\n",
      "print(z.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('C', 4), ('B', 3), ('A', 2), ('A', 1)]\n",
        "[('A', 8), ('B', 7), ('A', 6), ('D', 5)]\n",
        "[('A', (2, 8)), ('A', (2, 6)), ('A', (1, 8)), ('A', (1, 6)), ('B', (3, 7))]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.leftOuterJoin\">\n",
      "<img align=left src=\"files/images/pyspark-page48.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# leftOuterJoin\n",
      "x = sc.parallelize([('C',4),('B',3),('A',2),('A',1)])\n",
      "y = sc.parallelize([('A',8),('B',7),('A',6),('D',5)])\n",
      "z = x.leftOuterJoin(y)\n",
      "print(x.collect())\n",
      "print(y.collect())\n",
      "print(z.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('C', 4), ('B', 3), ('A', 2), ('A', 1)]\n",
        "[('A', 8), ('B', 7), ('A', 6), ('D', 5)]\n",
        "[('A', (2, 8)), ('A', (2, 6)), ('A', (1, 8)), ('A', (1, 6)), ('C', (4, None)), ('B', (3, 7))]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.rightOuterJoin\">\n",
      "<img align=left src=\"files/images/pyspark-page49.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# rightOuterJoin\n",
      "x = sc.parallelize([('C',4),('B',3),('A',2),('A',1)])\n",
      "y = sc.parallelize([('A',8),('B',7),('A',6),('D',5)])\n",
      "z = x.rightOuterJoin(y)\n",
      "print(x.collect())\n",
      "print(y.collect())\n",
      "print(z.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('C', 4), ('B', 3), ('A', 2), ('A', 1)]\n",
        "[('A', 8), ('B', 7), ('A', 6), ('D', 5)]\n",
        "[('A', (2, 8)), ('A', (2, 6)), ('A', (1, 8)), ('A', (1, 6)), ('B', (3, 7)), ('D', (None, 5))]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.partitionBy\">\n",
      "<img align=left src=\"files/images/pyspark-page50.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# partitionBy\n",
      "x = sc.parallelize([(0,1),(1,2),(2,3)],2)\n",
      "y = x.partitionBy(numPartitions = 3, partitionFunc = lambda x: x)  # only key is passed to paritionFunc\n",
      "print(x.glom().collect())\n",
      "print(y.glom().collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[(0, 1)], [(1, 2), (2, 3)]]\n",
        "[[(0, 1)], [(1, 2)], [(2, 3)]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.combineByKey\">\n",
      "<img align=left src=\"files/images/pyspark-page51.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# combineByKey\n",
      "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
      "createCombiner = (lambda el: [(el,el**2)]) \n",
      "mergeVal = (lambda aggregated, el: aggregated + [(el,el**2)]) # append to aggregated\n",
      "mergeComb = (lambda agg1,agg2: agg1 + agg2 )  # append agg1 with agg2\n",
      "y = x.combineByKey(createCombiner,mergeVal,mergeComb)\n",
      "print(x.collect())\n",
      "print(y.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('B', 1), ('B', 2), ('A', 3), ('A', 4), ('A', 5)]\n",
        "[('A', [(3, 9), (4, 16), (5, 25)]), ('B', [(1, 1), (2, 4)])]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 51
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.aggregateByKey\">\n",
      "<img align=left src=\"files/images/pyspark-page52.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# aggregateByKey\n",
      "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
      "zeroValue = [] # empty list is 'zero value' for append operation\n",
      "mergeVal = (lambda aggregated, el: aggregated + [(el,el**2)])\n",
      "mergeComb = (lambda agg1,agg2: agg1 + agg2 )\n",
      "y = x.aggregateByKey(zeroValue,mergeVal,mergeComb)\n",
      "print(x.collect())\n",
      "print(y.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('B', 1), ('B', 2), ('A', 3), ('A', 4), ('A', 5)]\n",
        "[('A', [(3, 9), (4, 16), (5, 25)]), ('B', [(1, 1), (2, 4)])]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.foldByKey\">\n",
      "<img align=left src=\"files/images/pyspark-page53.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# foldByKey\n",
      "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
      "zeroValue = 1 # one is 'zero value' for multiplication\n",
      "y = x.foldByKey(zeroValue,lambda agg,x: agg*x )  # computes cumulative product within each key\n",
      "print(x.collect())\n",
      "print(y.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('B', 1), ('B', 2), ('A', 3), ('A', 4), ('A', 5)]\n",
        "[('A', 60), ('B', 2)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.groupByKey\">\n",
      "<img align=left src=\"files/images/pyspark-page54.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# groupByKey\n",
      "x = sc.parallelize([('B',5),('B',4),('A',3),('A',2),('A',1)])\n",
      "y = x.groupByKey()\n",
      "print(x.collect())\n",
      "print([(j[0],[i for i in j[1]]) for j in y.collect()])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('B', 5), ('B', 4), ('A', 3), ('A', 2), ('A', 1)]\n",
        "[('A', [3, 2, 1]), ('B', [5, 4])]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.flatMapValues\">\n",
      "<img align=left src=\"files/images/pyspark-page55.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# flatMapValues\n",
      "x = sc.parallelize([('A',(1,2,3)),('B',(4,5))])\n",
      "y = x.flatMapValues(lambda x: [i**2 for i in x]) # function is applied to entire value, then result is flattened\n",
      "print(x.collect())\n",
      "print(y.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('A', (1, 2, 3)), ('B', (4, 5))]\n",
        "[('A', 1), ('A', 4), ('A', 9), ('B', 16), ('B', 25)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.mapValues\">\n",
      "<img align=left src=\"files/images/pyspark-page56.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# mapValues\n",
      "x = sc.parallelize([('A',(1,2,3)),('B',(4,5))])\n",
      "y = x.mapValues(lambda x: [i**2 for i in x]) # function is applied to entire value\n",
      "print(x.collect())\n",
      "print(y.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('A', (1, 2, 3)), ('B', (4, 5))]\n",
        "[('A', [1, 4, 9]), ('B', [16, 25])]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.groupWith\">\n",
      "<img align=left src=\"files/images/pyspark-page57.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# groupWith\n",
      "x = sc.parallelize([('C',4),('B',(3,3)),('A',2),('A',(1,1))])\n",
      "y = sc.parallelize([('B',(7,7)),('A',6),('D',(5,5))])\n",
      "z = sc.parallelize([('D',9),('B',(8,8))])\n",
      "a = x.groupWith(y,z)\n",
      "print(x.collect())\n",
      "print(y.collect())\n",
      "print(z.collect())\n",
      "print(\"Result:\")\n",
      "for key,val in list(a.collect()): \n",
      "    print(key, [list(i) for i in val])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('C', 4), ('B', (3, 3)), ('A', 2), ('A', (1, 1))]\n",
        "[('B', (7, 7)), ('A', 6), ('D', (5, 5))]\n",
        "[('D', 9), ('B', (8, 8))]\n",
        "Result:\n",
        "D"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [[], [(5, 5)], [9]]\n",
        "C [[4], [], []]\n",
        "B [[(3, 3)], [(7, 7)], [(8, 8)]]\n",
        "A [[2, (1, 1)], [6], []]\n"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.cogroup\">\n",
      "<img align=left src=\"files/images/pyspark-page58.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# cogroup\n",
      "x = sc.parallelize([('C',4),('B',(3,3)),('A',2),('A',(1,1))])\n",
      "y = sc.parallelize([('A',8),('B',7),('A',6),('D',(5,5))])\n",
      "z = x.cogroup(y)\n",
      "print(x.collect())\n",
      "print(y.collect())\n",
      "for key,val in list(z.collect()):\n",
      "    print(key, [list(i) for i in val])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('C', 4), ('B', (3, 3)), ('A', 2), ('A', (1, 1))]\n",
        "[('A', 8), ('B', 7), ('A', 6), ('D', (5, 5))]\n",
        "A"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [[2, (1, 1)], [8, 6]]\n",
        "C [[4], []]\n",
        "B [[(3, 3)], [7]]\n",
        "D [[], [(5, 5)]]\n"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.sampleByKey\">\n",
      "<img align=left src=\"files/images/pyspark-page59.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# sampleByKey\n",
      "x = sc.parallelize([('A',1),('B',2),('C',3),('B',4),('A',5)])\n",
      "y = x.sampleByKey(withReplacement=False, fractions={'A':0.5, 'B':1, 'C':0.2})\n",
      "print(x.collect())\n",
      "print(y.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('A', 1), ('B', 2), ('C', 3), ('B', 4), ('A', 5)]\n",
        "[('B', 2), ('C', 3), ('B', 4)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.subtractByKey\">\n",
      "<img align=left src=\"files/images/pyspark-page60.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# subtractByKey\n",
      "x = sc.parallelize([('C',1),('B',2),('A',3),('A',4)])\n",
      "y = sc.parallelize([('A',5),('D',6),('A',7),('D',8)])\n",
      "z = x.subtractByKey(y)\n",
      "print(x.collect())\n",
      "print(y.collect())\n",
      "print(z.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('C', 1), ('B', 2), ('A', 3), ('A', 4)]\n",
        "[('A', 5), ('D', 6), ('A', 7), ('D', 8)]\n",
        "[('C', 1), ('B', 2)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.subtract\">\n",
      "<img align=left src=\"files/images/pyspark-page61.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# subtract\n",
      "x = sc.parallelize([('C',4),('B',3),('A',2),('A',1)])\n",
      "y = sc.parallelize([('C',8),('A',2),('D',1)])\n",
      "z = x.subtract(y)\n",
      "print(x.collect())\n",
      "print(y.collect())\n",
      "print(z.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('C', 4), ('B', 3), ('A', 2), ('A', 1)]\n",
        "[('C', 8), ('A', 2), ('D', 1)]\n",
        "[('A', 1), ('C', 4), ('B', 3)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.keyBy\">\n",
      "<img align=left src=\"files/images/pyspark-page62.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# keyBy\n",
      "x = sc.parallelize([1,2,3])\n",
      "y = x.keyBy(lambda x: x**2)\n",
      "print(x.collect())\n",
      "print(y.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 2, 3]\n",
        "[(1, 1), (4, 2), (9, 3)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 62
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.repartition\">\n",
      "<img align=left src=\"files/images/pyspark-page63.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# repartition\n",
      "x = sc.parallelize([1,2,3,4,5],2)\n",
      "y = x.repartition(numPartitions=3)\n",
      "print(x.glom().collect())\n",
      "print(y.glom().collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[1, 2], [3, 4, 5]]\n",
        "[[], [1, 2, 3, 4], [5]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 63
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.coalesce\">\n",
      "<img align=left src=\"files/images/pyspark-page64.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# coalesce\n",
      "x = sc.parallelize([1,2,3,4,5],2)\n",
      "y = x.coalesce(numPartitions=1)\n",
      "print(x.glom().collect())\n",
      "print(y.glom().collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[1, 2], [3, 4, 5]]\n",
        "[[1, 2, 3, 4, 5]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.zip\">\n",
      "<img align=left src=\"files/images/pyspark-page65.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# zip\n",
      "x = sc.parallelize(['B','A','A'])\n",
      "y = x.map(lambda x: ord(x))  # zip expects x and y to have same #partitions and #elements/partition\n",
      "z = x.zip(y)\n",
      "print(x.collect())\n",
      "print(y.collect())\n",
      "print(z.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['B', 'A', 'A']\n",
        "[66, 65, 65]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[('B', 66), ('A', 65), ('A', 65)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 65
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.zipWithIndex\">\n",
      "<img align=left src=\"files/images/pyspark-page66.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# zipWithIndex\n",
      "x = sc.parallelize(['B','A','A'],2)\n",
      "y = x.zipWithIndex()\n",
      "print(x.glom().collect())\n",
      "print(y.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[['B'], ['A', 'A']]\n",
        "[('B', 0), ('A', 1), ('A', 2)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.zipWithUniqueId\">\n",
      "<img align=left src=\"files/images/pyspark-page67.svg\" width=500 height=500 />\n",
      "</a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# zipWithUniqueId\n",
      "x = sc.parallelize(['B','A','A'],2)\n",
      "y = x.zipWithUniqueId()\n",
      "print(x.glom().collect())\n",
      "print(y.collect())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[['B'], ['A', 'A']]\n",
        "[('B', 0), ('A', 1), ('A', 3)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 67
    }
   ],
   "metadata": {}
  }
 ]
}